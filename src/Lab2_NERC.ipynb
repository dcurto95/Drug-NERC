{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab AHLT Drug NER - Part 2\n",
    "### Authors: David Curto & David Hilazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second part of the deliverable which is based on building a NERC using Machine Learning that recognizes and classifies drug names in a text. In this second laboratory there are 2 goals that must be achieved:\n",
    "\n",
    "1. Implement a NERC using machine learning and get an overall F1 score of at least **0.6** on Devel dataset using only information from the training dataset. \n",
    "\n",
    "2. Implement a NERC using machine learning and get an overall F1 score of at least **0.7** on Devel dataset using also external knowledge sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import pycrfsuite\n",
    "from chemdataextractor.nlp.tokenize import ChemWordTokenizer\n",
    "from nltk import word_tokenize, QuadgramCollocationFinder\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read and extract info from the XML file**\n",
    "\n",
    "This first function reads the XML file using the The ElementTree XML API and returns the root of the tree that will be used to parse all the information needed.\n",
    "\n",
    "The second one retrieves the information that will be used in the following functions which is the id of the sentence and the text that it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(file):\n",
    "    tree = ET.parse(file)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_sentence_info(child):\n",
    "    return child.get('id'), child.get('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizers**\n",
    "\n",
    "\n",
    "Once we have the sentence text we have to apply some transformations in order to be able to manipulate them easily. This first preprocessing step is called  word Tokenization which splits the sentence into tokens. \n",
    "\n",
    "In order to do that, we have tested two different tokenizers. The first word tokenizer tested is one of the most well known in NLP task which is the NLTK word tokenizer. However, also we tested one implemented by a tool named ChemDataExtractor which is designed for automatically extracting chemical information from scientific documents. As this tokenizer is more field oriented, it handles better some chemical word formations than the NLTK word tokenizer which is less field oriented.\n",
    "\n",
    "For this reason, we decided to just use the ChemWordTokenizer as it retrieves better the tokens for specific cases. \n",
    "\n",
    "Both of these function returns all the tokens founds and its starting and ending position for each one of them within the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chem_tokenize(text):\n",
    "    cwt = ChemWordTokenizer()\n",
    "    tokens = cwt.tokenize(text)\n",
    "    token_indexs = cwt.span_tokenize(text)\n",
    "    tokenized_info = []\n",
    "    for token_index, token in zip(token_indexs, tokens):\n",
    "        tokenized_info.append((token, token_index[0], token_index[1] - 1))\n",
    "    return tokenized_info\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenized_sent = word_tokenize(text)\n",
    "    tokenized_info = []\n",
    "    current_index = 0\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "\n",
    "        if not re.match(\"[\" + string.punctuation + \"]\", word):\n",
    "            for match in re.finditer(word, text):\n",
    "                if match.start() >= current_index:\n",
    "                    tokenized_info.append((word, match.start(), match.end() - 1))\n",
    "                    current_index = match.end() - 1\n",
    "                    break\n",
    "    return tokenized_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get external resources**\n",
    "\n",
    "This function reads the external file named DrugBank and stores its information (name , type of drug) as in the first part of the laboratory. However in this second part we have also used the HSDB file and csv file that we have generated that contains all drug_n names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_external_resources():\n",
    "    file = open('../resources/DrugBank.txt', 'r', encoding=\"utf8\")\n",
    "    Lines = file.readlines()\n",
    "\n",
    "    resources = {}\n",
    "\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        value = line.split(\"|\")\n",
    "        resources[value[0].lower()] = value[1][:-1]\n",
    "\n",
    "    hsdb_resources = set()\n",
    "    file = open('../resources/HSDB.txt', 'r', encoding=\"utf8\")\n",
    "    Lines = file.readlines()\n",
    "\n",
    "    for line in Lines:\n",
    "        value = line[:-1]\n",
    "        hsdb_resources.add(value.lower())\n",
    "\n",
    "    drug_n_resources = set()\n",
    "    file = open('../resources/drug_n.csv', 'r', encoding=\"utf8\")\n",
    "    Lines = file.readlines()\n",
    "\n",
    "    for line in Lines:\n",
    "        value = line[:-1]\n",
    "        drug_n_resources.add(value.lower())\n",
    "\n",
    "    return resources, hsdb_resources, drug_n_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract features**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(token_list, entities_dict, drug_n_set, hsdb_set, with_resources=False):\n",
    "    entities = []\n",
    "    previous_token_offset = (0, 0)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # TODO: Create features for any class (punct, nomes numeros, ...)\n",
    "    for i, token in enumerate(token_list):\n",
    "        features = []\n",
    "        if entities_dict and with_resources:\n",
    "            if token[0] in entities_dict:\n",
    "                features.append(\"in_entities_dict=\" + entities_dict[token[0]])\n",
    "            elif token[0].lower() in entities_dict:\n",
    "                features.append(\"in_entities_dict=\" + entities_dict[token[0].lower()])\n",
    "            else:\n",
    "                features.append(\"in_entities_dict=False\")\n",
    "        else:\n",
    "            features.append(\"in_entities_dict=False\")\n",
    "        \n",
    "        #Check if token belongs to drug_n (external file)\n",
    "        if with_resources and token[0] in drug_n_set:\n",
    "            features.append(\"is_drug_n=True\")\n",
    "        elif with_resources and token[0].lower() in drug_n_set:\n",
    "            features.append(\"is_drug_n=True\")\n",
    "        else:\n",
    "            features.append(\"is_drug_n=False\")\n",
    "        \n",
    "        \n",
    "        #Check if token is inside hsdb file  (external file)\n",
    "        if with_resources and token[0] in hsdb_set:\n",
    "            features.append(\"in_hsdb_set=True\")\n",
    "        elif with_resources and token[0].lower() in hsdb_set:\n",
    "            features.append(\"in_hsdb_set=True\")\n",
    "        else:\n",
    "            features.append(\"in_hsdb_set=False\")\n",
    "        \n",
    "        \n",
    "        lower_letters = sum(1 for c in token[0] if c.isupper())\n",
    "        upper_letters = sum(1 for c in token[0] if c.isupper())\n",
    "        num_digits = sum(1 for c in token[0] if c.isdigit())\n",
    "        num_punctuation = sum(1 for c in token[0] if c in string.punctuation)\n",
    "        num_roman = len(re.findall(\"[IVXDLCM]+\", token[0]))\n",
    "        \n",
    "        # Custom feature that counts the amount of upper and lowercase letters\n",
    "        # the number of digits , punctuation and roman characters\n",
    "        word_shape = lower_letters + upper_letters + num_digits + num_punctuation + num_roman\n",
    "        features.append(\"word_shape=\" + str(word_shape))\n",
    "        \n",
    "        # Check if token starts with a digit\n",
    "        features.append(\"starts_with_digit=\" + str(token[0][0].isdigit()))\n",
    "        \n",
    "        # Check if token belongs to the stop word corpus, only available with resources\n",
    "        features.append(\"is_stopword=\" + str(token[0].lower() in stop_words and with_resources))\n",
    "        \n",
    "        \n",
    "        # Check if token belongs to the stop word set\n",
    "        stopword_set = {'of', 'the', 'and', 'in', 'with', 'to', 'be', 'or', 'is', 'not', 'by', 'for',\n",
    "                        'should', 'on', 'that', 'been', 'have', 'other', 'was', 'when', 'are', 'as', 'were',\n",
    "                        'no', 'has', 'these', 'an', 'this', 'such', 'at', 'from', 'it', 'if', 'there', 'after',\n",
    "                        'which', 'can', 'between', 'during', 'because', 'both', 'than', 'did', 'its', 'but',\n",
    "                        'some', 'who', 'any'}\n",
    "        features.append(\"in_stopword_set=\" + str(token[0].lower() in stopword_set))\n",
    "        \n",
    "        #Check if token has parenthesis and it's size is larger than 1 \n",
    "        features.append(\"has_parenthesis=\" + str('(' in token[0] and len(token[0]) > 1))\n",
    "        \n",
    "        #Check if token is a punctuation character\n",
    "        features.append(\"is_punct=\" + str(token[0] in {'.', ',', ';', ':', '(', ')', '-', '_', '\\'', '/', '\\\\'}))\n",
    "        \n",
    "        #Check if token has lowercase letters separated by a hyphen\n",
    "        features.append(\"has_lowercase_hyphen=\" + str(bool(re.search(\"[a-z][\\-][a-z]\", token[0]))))\n",
    "        \n",
    "        #Ckeck if token is only formed by numbers\n",
    "        features.append(\n",
    "            \"is_only_numbers=\" + str(re.search(\"^(\\d+[\\-\\.]\\d+)$|^(\\d+\\.\\d+\\-\\d+\\.\\d+)$\", token[0]) is None))\n",
    "\n",
    "        #Check if contains a hyphen\n",
    "        features.append(\"has_hyphen=\" + str(bool(re.search(\"\\w[_%()\\-]\\w\", token[0]))))\n",
    "        \n",
    "        #Check if token is in uppercase\n",
    "        features.append(\"is_upper=\" + str(token[0].isupper()))\n",
    "    \n",
    "        #Check if token contains an uppercase vowel\n",
    "        pattern = re.compile(\"[AEIOU]\")\n",
    "        features.append(\"has_upper_vowel=\" + str(bool(pattern.search(token[0]))))\n",
    "        \n",
    "        #Check if token contains any common word of the different types\n",
    "        features.append(\n",
    "            \"has_group_keyword=\" + str(bool(len(entities) > 0 and previous_token_offset[1] + 2 == token[1] and any(\n",
    "                substring in token[0].lower() for substring in\n",
    "                ['agent', 'inhibitor', 'blocker', 'drug', 'type', 'medication', 'contraceptive', 'anticoagulants']))))\n",
    "\n",
    "        has_common_drug = token[0].lower() in ['digoxin', 'warfarin', 'phenytoin', 'theophylline', 'lithium',\n",
    "                                               'ketoconazole', 'cimetidine', 'alcohol', 'cyclosporine', 'erythromycin',\n",
    "                                               'tricyclic antidepressants', 'aspirin', 'carbamazepine', 'rifampin',\n",
    "                                               'amiodarone', 'quinidine', 'phenobarbital', 'indinavir', 'propranolol',\n",
    "                                               'methotrexate', 'diltiazem', 'cisapride', 'ethanol']\n",
    "        features.append(\"has_common_drug=\" + str(has_common_drug))\n",
    "\n",
    "        features.append(\"has_common_group=\" + str(any(substring in token[0].lower() for substring in\n",
    "                                                      ['anticoagulant', 'corticosteroid', 'NSAID', 'antacid',\n",
    "                                                       'contraceptive', 'diuretic', 'barbiturate'])))\n",
    "        \n",
    "        #Add features of prefixes and suffixes of different length(2-4)\n",
    "        if len(token[0]) >= 4:\n",
    "            features.append(\"suff4=\" + token[0][-4:])\n",
    "            features.append(\"pref4=\" + token[0][:4])\n",
    "        else:\n",
    "            features.append(\"suff4=\" + token[0])\n",
    "            features.append(\"pref4=\" + token[0])\n",
    "\n",
    "        if len(token[0]) >= 3:\n",
    "            features.append(\"suff3=\" + token[0][-3:])\n",
    "            features.append(\"pref3=\" + token[0][:3])\n",
    "        else:\n",
    "            features.append(\"suff3=\" + token[0])\n",
    "            features.append(\"pref3=\" + token[0])\n",
    "\n",
    "        if len(token[0]) >= 2:\n",
    "            features.append(\"suff2=\" + token[0][-2:])\n",
    "            features.append(\"pref2=\" + token[0][:2])\n",
    "        else:\n",
    "            features.append(\"suff2=\" + token[0])\n",
    "            features.append(\"pref2=\" + token[0])\n",
    "        \n",
    "        # Check if contains POC inside the token (characteristic word)\n",
    "        features.append(\"has_poc=\" + str(\"POC\" in token[0]))\n",
    "\n",
    "        features.append(\"starts_with_uppercase=\" + str(token[0][0].isupper()))\n",
    "        \n",
    "        # Information from previous token\n",
    "        if i != 0:\n",
    "            features.append(\"prev_ent=\" + token_list[i - 1][0])\n",
    "            features.append(\"prev_postag=\" + nltk.pos_tag([token_list[i - 1][0]])[0][1][0])\n",
    "            features.append(\"prev_len=\" + str(len(token_list[i - 1][0])))\n",
    "        else:\n",
    "            features.append(\"prev_ent=\")\n",
    "            features.append(\"prev_postag=\")\n",
    "            features.append(\"prev_len=0\")\n",
    "        \n",
    "        #Information from current token\n",
    "        features.append(\"curr_ent=\" + token_list[i][0])\n",
    "        features.append(\"curr_postag=\" + nltk.pos_tag([token[0]])[0][1][0])\n",
    "        features.append(\"curr_len=\" + str(len(token_list[i][0])))\n",
    "        \n",
    "        #Information from next token\n",
    "        if i != len(token_list) - 1:\n",
    "            features.append(\"next_ent=\" + token_list[i + 1][0])\n",
    "            features.append(\"next_postag=\" + nltk.pos_tag([token_list[i + 1][0]])[0][1][0])\n",
    "            features.append(\"next_len=\" + str(len(token_list[i + 1][0])))\n",
    "        else:\n",
    "            features.append(\"next_ent=\")\n",
    "            features.append(\"next_postag=\")\n",
    "            features.append(\"next_len=0\")\n",
    "        \n",
    "        # Check if there are uppercase letters (without taking into account the start)\n",
    "        features.append(\"capitals_inside=\" + str(token[0][1:].lower() != token[0][1:]))\n",
    "        \n",
    "        # Count number of punctuation characters\n",
    "        features.append(\"count_punct=\" + str(len(re.findall(\"[\\.\\-+\\,()]\", token[0]))))\n",
    "        \n",
    "        # Check if token contains roman letters\n",
    "        features.append(\"has_roman=\" + str(bool(re.search(\"[IVXDLCM]+\", token[0]))))\n",
    "\n",
    "        entities.append(features)\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output features and entities**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_features(sid, tokens, gold_entities, features, output_file):\n",
    "    for token, feature_vector, bio in zip(tokens, features, gold_entities):\n",
    "        output_file.write(\n",
    "            sid + \"\\t\" + token[0] + \"\\t\" + str(token[1]) + \"\\t\" + str(token[2]) + \"\\t\" + bio + \"\\t\" + \"\\t\".join(\n",
    "                feature_vector) + \"\\n\")\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def output_entities(sid, entities, output_file):\n",
    "    for entity in entities:\n",
    "        output_file.write(sid + \"|\" + entity['offset'] + \"|\" + entity['name'] + \"|\" + entity['type'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get truth entities**\n",
    "\n",
    "TODO: Ground truth! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truth_entities(child):\n",
    "    return list(zip([ent.get('text') for ent in child.findall('entity')],\n",
    "                    [ent.get('charOffset') for ent in child.findall('entity')])), \\\n",
    "           [ent.get('type') for ent in child.findall('entity')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get gold entities**\n",
    "\n",
    "TODO: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_entities(token_list, truth_entities):\n",
    "    gold_entities = []\n",
    "    entity_counter = 0\n",
    "\n",
    "    for j, token in enumerate(token_list):\n",
    "        if not truth_entities or len(truth_entities[0]) <= entity_counter:\n",
    "            gold_entities.append(\"O\")\n",
    "            continue\n",
    "        if ';' in truth_entities[0][entity_counter][1]:\n",
    "            entity_offsets = truth_entities[0][entity_counter][1].split(';')\n",
    "            for i, offset in enumerate(entity_offsets):\n",
    "                entity_offset = offset.split('-')\n",
    "                entity_offset[0] = int(entity_offset[0])\n",
    "                entity_offset[1] = int(entity_offset[1])\n",
    "                entity_offsets[i] = (entity_offset[0], entity_offset[1])\n",
    "        else:\n",
    "            entity_offset = truth_entities[0][entity_counter][1].split('-')\n",
    "            entity_offset[0] = int(entity_offset[0])\n",
    "            entity_offset[1] = int(entity_offset[1])\n",
    "            entity_offsets = [(entity_offset[0], entity_offset[1])]\n",
    "\n",
    "        for offset in entity_offsets:\n",
    "            if offset[0] == token[1] and offset[1] == token[2]:\n",
    "                # Exact match\n",
    "                gold_entities.append(\"B-\" + truth_entities[1][entity_counter])\n",
    "                entity_counter += 1\n",
    "                break\n",
    "            elif offset[0] == token[1]:\n",
    "                # Beginning match\n",
    "                gold_entities.append(\"B-\" + truth_entities[1][entity_counter])\n",
    "                break\n",
    "            elif offset[0] < token[1] and offset[1] > token[2]:\n",
    "                # Inside match\n",
    "                gold_entities.append(\"I-\" + truth_entities[1][entity_counter])\n",
    "                break\n",
    "            elif offset[1] == token[2]:\n",
    "                # End match\n",
    "                gold_entities.append(\"I-\" + truth_entities[1][entity_counter])\n",
    "                entity_counter += 1\n",
    "                break\n",
    "        if len(gold_entities) == j:\n",
    "            gold_entities.append('O')\n",
    "\n",
    "    return gold_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Read features**\n",
    "\n",
    "Reads the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(filename):\n",
    "    # Using readlines()\n",
    "    file1 = open('../output/' + filename, 'r')\n",
    "    lines = file1.readlines()\n",
    "\n",
    "    sentences_features = []\n",
    "    sentences_bios = []\n",
    "\n",
    "    token_features = []\n",
    "    bios = []\n",
    "\n",
    "    # Strips the newline character\n",
    "    for line in lines:\n",
    "        if line == \"\\n\":\n",
    "            # End of sentence\n",
    "            sentences_features.append(token_features)\n",
    "            sentences_bios.append(bios)\n",
    "\n",
    "            token_features = []\n",
    "            bios = []\n",
    "            continue\n",
    "        values = line.split(\"\\t\")\n",
    "\n",
    "        sid = values[0]\n",
    "        token_name = values[1]\n",
    "        start_offset = values[2]\n",
    "        end_offset = values[3]\n",
    "        bio = values[4]\n",
    "        feature_vector = values[5:]\n",
    "        feature_vector[-1].replace('\\n', '')\n",
    "\n",
    "        bios.append(bio)\n",
    "        token_features.append(feature_vector)\n",
    "\n",
    "    return sentences_features, sentences_bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_bio(token_list, bio_tags):\n",
    "    entities = []\n",
    "    previous_token_offset = (0, 0)\n",
    "\n",
    "    for token, bio in zip(token_list, bio_tags):\n",
    "        if bio.startswith('B'):\n",
    "            entities.append({'name': token[0],\n",
    "                             'offset': str(token[1]) + \"-\" + str(token[2]),\n",
    "                             'type': bio.split(\"-\")[-1]})\n",
    "            previous_token_offset = (token[1], token[2])\n",
    "        elif bio.startswith('I') and entities:\n",
    "            entities[-1]['name'] += \" \" + token[0]\n",
    "            entities[-1]['offset'] = str(previous_token_offset[0]) + \"-\" + str(token[2])\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputdir, outputfile):\n",
    "    return os.system(\"java -jar ../eval/evaluateNER.jar \" + inputdir + \" ../output/\" + outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(with_resources):\n",
    "    # Get Features:\n",
    "    output_file_name = \"features.txt\"\n",
    "    input_directory = '../data/Train/'\n",
    "\n",
    "    entities_dict, drug_n_set, hsdb_set = get_external_resources()\n",
    "\n",
    "    output_file = open('../output/' + output_file_name, 'w+')\n",
    "    for filename in os.listdir(input_directory):\n",
    "        root = parse_xml(input_directory + filename)\n",
    "        for child in root:\n",
    "            sid, text = get_sentence_info(child)\n",
    "            token_list = chem_tokenize(text)\n",
    "            features = extract_features(token_list, entities_dict, drug_n_set, hsdb_set,\n",
    "                                        with_resources=with_resources)\n",
    "            truth_entities = get_truth_entities(child)\n",
    "            gold_entities = get_gold_entities(token_list, truth_entities)\n",
    "            output_features(sid, token_list, gold_entities, features, output_file)\n",
    "\n",
    "    # Close the file\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf(with_resources):  \n",
    "    \n",
    "    # Generate and save features\n",
    "    save_features(with_resources)\n",
    "    \n",
    "    # Train CRF:\n",
    "    sentences_features, sentences_bios = read_features(\"features.txt\")\n",
    "    \n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "    for xseq, yseq in zip(sentences_features, sentences_bios):\n",
    "        trainer.append(xseq, yseq)\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': 1.0,  # coefficient for L1 penalty\n",
    "        'c2': 1e-3,  # coefficient for L2 penalty\n",
    "        'max_iterations': 50,  # stop earlier\n",
    "\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "    trainer.train('crf_model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_crf(with_resources):\n",
    "    # Evaluate Devel\n",
    "    output_file_name = \"task9.1_out_2.txt\"\n",
    "    input_directory = '../data/Devel/'\n",
    "\n",
    "    entities_dict, drug_n_set, hsdb_set = get_external_resources()\n",
    "\n",
    "    output_file = open('../output/' + output_file_name, 'w+')\n",
    "    for filename in os.listdir(input_directory):\n",
    "        root = parse_xml(input_directory + filename)\n",
    "        for child in root:\n",
    "            sid, text = get_sentence_info(child)\n",
    "            token_list = chem_tokenize(text)\n",
    "            features = extract_features(token_list, entities_dict, drug_n_set, hsdb_set,\n",
    "                                        with_resources=with_resources)\n",
    "\n",
    "            tagger = pycrfsuite.Tagger()\n",
    "            tagger.open('crf_model.crfsuite')\n",
    "            bio_tags = tagger.tag(features)\n",
    "\n",
    "            entities = extract_entities_from_bio(token_list, bio_tags)\n",
    "            output_entities(sid, entities, output_file)\n",
    "\n",
    "    # Close the file\n",
    "    output_file.close()\n",
    "    evaluate(input_directory, output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_resources = False\n",
    "train_crf(with_resources)\n",
    "evaluate_crf(with_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_resources = True\n",
    "train_crf(with_resources)\n",
    "evaluate_crf(with_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Missing and Failed entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING:\n",
      " {\n",
      "    \"drug\": [\n",
      "        \"antibiotics\",\n",
      "        \"gentamicin\",\n",
      "        \"99mTc-MDP\",\n",
      "        \"99mTc-MDP\",\n",
      "        \"25-Dihydroxycholecalciferol D3\",\n",
      "        \"1,25(OH)2D3\",\n",
      "        \"1,25(OH)2D3\",\n",
      "        \"1,25(OH)2D3\",\n",
      "        \"norepinephrine\",\n",
      "        \"norepinephrine\",\n",
      "        \"norepinephrine\",\n",
      "        \"EACA\",\n",
      "        \"erythromycins\",\n",
      "        \"amyl nitrite\",\n",
      "        \"drugs\",\n",
      "        \"toxin\",\n",
      "        \"antiarrhythmics\",\n",
      "        \"Calcidiol\",\n",
      "        \"Calcium\",\n",
      "        \"calcium\",\n",
      "        \"aluminum\",\n",
      "        \"Niacin\",\n",
      "        \"Nicotinic Acid\",\n",
      "        \"Magnesium-Aluminum Hydroxide\",\n",
      "        \"CIMETlDINE\",\n",
      "        \"ACTH\",\n",
      "        \"thiazide diuretics\",\n",
      "        \"Calcidiol\",\n",
      "        \"Calcium\",\n",
      "        \"calcium\",\n",
      "        \"clorazepate dipotassium\",\n",
      "        \"TAO\",\n",
      "        \"CBZ\",\n",
      "        \"Potassium\",\n",
      "        \"amiloride\",\n",
      "        \"triamterene\",\n",
      "        \"Caffeine\",\n",
      "        \"Theobromine\",\n",
      "        \"Grepafloxacin\",\n",
      "        \"ACTH\",\n",
      "        \"Human growth hormone\",\n",
      "        \"human growth hormone\",\n",
      "        \"pargyline\",\n",
      "        \"SMX\",\n",
      "        \"tricyclic antidepressants\",\n",
      "        \"antihypertensives\",\n",
      "        \"terfenadine\"\n",
      "    ],\n",
      "    \"drug_n\": [\n",
      "        \"KRM-1648\",\n",
      "        \"benzoxazinorifamycin\",\n",
      "        \"KRM-1648\",\n",
      "        \"KRM-1648\",\n",
      "        \"cerulein\",\n",
      "        \"3H-spiroperidol\",\n",
      "        \"pyreneperone\",\n",
      "        \"3H-spiroperidol\",\n",
      "        \"3H-spiroperidol\",\n",
      "        \"cerulein\",\n",
      "        \"3H-spiroperidol\",\n",
      "        \"3H-spiroperidol\",\n",
      "        \"spermidine\",\n",
      "        \"putrescine\",\n",
      "        \"spermidine\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"N-allylnormetazocine\",\n",
      "        \"NANM\",\n",
      "        \"(+)-NANM\",\n",
      "        \"(-)-NANM\",\n",
      "        \"(+)-NANM\",\n",
      "        \"(-)-NANM\",\n",
      "        \"(+)-NANM\",\n",
      "        \"(-)-NANM\",\n",
      "        \"(-)-NANM\",\n",
      "        \"(+)-NANM\",\n",
      "        \"(+)-NANM\",\n",
      "        \"NANM\",\n",
      "        \"4-hydroxytamoxifen\",\n",
      "        \"TAM\",\n",
      "        \"copper sulfate\",\n",
      "        \"hydroxybupropion\",\n",
      "        \"threohydrobupropion\",\n",
      "        \"hydroxybupropion\",\n",
      "        \"hydroxybupropion\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"Clofibric acid\",\n",
      "        \"clofibric acid\",\n",
      "        \"carbamazepine epoxide\",\n",
      "        \"Carbamazepine epoxide\",\n",
      "        \"carbamazepine epoxide\",\n",
      "        \"fenofibric acid\",\n",
      "        \"fenofibric acid\",\n",
      "        \"fenofibric acid\",\n",
      "        \"fosinoprilat\",\n",
      "        \"fosinoprilat\",\n",
      "        \"fosinoprilat\"\n",
      "    ],\n",
      "    \"group\": [\n",
      "        \"SSRIs\",\n",
      "        \"nitrate\",\n",
      "        \"calcium-channel blockers\",\n",
      "        \"Slow-channel calcium blockers\",\n",
      "        \"radiopharmaceutical\",\n",
      "        \"antiestrogen\",\n",
      "        \"vitamin-D\",\n",
      "        \"immunodepressant\",\n",
      "        \"Bacteriostatic Antibiotics\",\n",
      "        \"5HT3 Antagonists\",\n",
      "        \"5HT3 antagonist class\",\n",
      "        \"Antihypertensive Medication\",\n",
      "        \"Vasodilators\",\n",
      "        \"Macrolides\",\n",
      "        \"beta-blocking agent\",\n",
      "        \"HMG-CoA reductase inhibitors\",\n",
      "        \"HMG-CoA reductase inhibitors\",\n",
      "        \"HMG-CoA reductase inhibitor\",\n",
      "        \"macrolide products\",\n",
      "        \"curare-like compounds\",\n",
      "        \"botulinum toxin\",\n",
      "        \"steroids\",\n",
      "        \"Iron Supplements\",\n",
      "        \"iron supplement\",\n",
      "        \"iron supplements\",\n",
      "        \"Immunosuppressive Drugs\",\n",
      "        \"Azole Antifungals\",\n",
      "        \"ANTACID\",\n",
      "        \"angiotensin- converting enzyme (ACE) inhibitors\",\n",
      "        \"calcium-channel blockers\",\n",
      "        \"diuretic\",\n",
      "        \"muscle relaxant\",\n",
      "        \"loop diuretics\",\n",
      "        \"potassium-sparing diuretics\",\n",
      "        \"vitamin D\",\n",
      "        \"vitamin D\",\n",
      "        \"vitamin D\",\n",
      "        \"coagulation factor\",\n",
      "        \"HMG-CoA reductase inhibitor\",\n",
      "        \"dopa decarboxylase inhibitor\",\n",
      "        \"dopa decarboxylase inhibitor\",\n",
      "        \"dopa decarboxylase inhibitor\",\n",
      "        \"coumarin-derivative anticoagulants\",\n",
      "        \"synthetic estrogens\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"progestin\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"synthetic estrogens\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"hormonal contraceptives\",\n",
      "        \"AED\",\n",
      "        \"AED\",\n",
      "        \"Combination Oral Contraceptives\",\n",
      "        \"COUMARIN ANTICOAGULANTS\",\n",
      "        \"HMG-CoA reductase inhibitors\",\n",
      "        \"HMG-CoA reductase inhibitors\",\n",
      "        \"Resins\",\n",
      "        \"bile acid sequestrants\",\n",
      "        \"bile acid binding resin\",\n",
      "        \"coumarin-type anticoagulants\",\n",
      "        \"bile acid binding resin\",\n",
      "        \"cytostatic agent\",\n",
      "        \"Potassium-Sparing Diuretics\",\n",
      "        \"Potassium-sparing diuretics\",\n",
      "        \"histamine H2-receptor antagonists\",\n",
      "        \"Multivitamins\",\n",
      "        \"nonsteroidal anti inflammatory drug\",\n",
      "        \"anesthetic\",\n",
      "        \"skeletal-muscle relaxants\",\n",
      "        \"loop diuretics\",\n",
      "        \"potassium sparing diuretics\",\n",
      "        \"benzodiazepines\",\n",
      "        \"HMG-CoA reductase inhibitors\",\n",
      "        \"loop diuretics\",\n",
      "        \"potassium-sparing diuretics\",\n",
      "        \"potassium-sparing diuretics\",\n",
      "        \"potassium-sparing diuretics\",\n",
      "        \"MAO inhibitors\",\n",
      "        \"ergot-type oxytocic drugs\",\n",
      "        \"Adrenergic blockers\",\n",
      "        \"Adrenergic blockers\",\n",
      "        \"tricyclic\",\n",
      "        \"Amphetamines\",\n",
      "        \"MAO inhibitors\",\n",
      "        \"MAOI antidepressants\",\n",
      "        \"amphetamine\",\n",
      "        \"coumarin-type anticoagulants\",\n",
      "        \"hydantoin\",\n",
      "        \"sulfonamide\",\n",
      "        \"ergot derivatives\",\n",
      "        \"Antimycobacterial agents\",\n",
      "        \"Benzodiazepines\"\n",
      "    ],\n",
      "    \"brand\": [\n",
      "        \"sandimmune\",\n",
      "        \"sandimmune\",\n",
      "        \"KEMSTROTM\",\n",
      "        \"TRANXENE\",\n",
      "        \"Felbatol\",\n",
      "        \"OVCON-35\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "WRONG:\n",
      " {\n",
      "    \"drug\": [\n",
      "        \"noradrenaline\",\n",
      "        \"nitrate\",\n",
      "        \"norepinephrine\",\n",
      "        \"cholecystokinin octapeptide\",\n",
      "        \"Cholecystokinin octapeptide\",\n",
      "        \"spermidine\",\n",
      "        \"putrescine\",\n",
      "        \"spermidine\",\n",
      "        \"arachidonic acid\",\n",
      "        \"arachidonic acid\",\n",
      "        \"calcium ions\",\n",
      "        \"arachidonic acid\",\n",
      "        \"calcium ions\",\n",
      "        \"Adenosine\",\n",
      "        \"arachidonic acid\",\n",
      "        \"N-allylnormetazocine\",\n",
      "        \"sandimmune\",\n",
      "        \"ATP\",\n",
      "        \"fibrinogen\",\n",
      "        \"macrolide\",\n",
      "        \"calcium\",\n",
      "        \"Calcium Supplements\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"iron\",\n",
      "        \"Magnesium\",\n",
      "        \"Aluminum Hydroxide\",\n",
      "        \"calcium\",\n",
      "        \"Calcium Supplements\",\n",
      "        \"clorazepate\",\n",
      "        \"beta-glucuronidase\",\n",
      "        \"rifamipicin\",\n",
      "        \"Clofibric acid\",\n",
      "        \"clofibric acid\",\n",
      "        \"carbamazepine epoxide\",\n",
      "        \"Carbamazepine epoxide\",\n",
      "        \"carbamazepine epoxide\",\n",
      "        \"COUMARIN ANTICOAGULANTS\",\n",
      "        \"PROTHROMBIN\",\n",
      "        \"PROTHROMBIN\",\n",
      "        \"PROTHROMBIN\",\n",
      "        \"bile acid sequestrants\",\n",
      "        \"fenofibric acid\",\n",
      "        \"-hydroxy-iso-pravastatin\",\n",
      "        \"fenofibric acid\",\n",
      "        \"fenofibric acid\",\n",
      "        \"lithium\",\n",
      "        \"fosinoprilat\",\n",
      "        \"fosinoprilat\",\n",
      "        \"fosinoprilat\",\n",
      "        \"Caffeine Theobromine Grepafloxacin\",\n",
      "        \"magnesium\",\n",
      "        \"magnesium\",\n",
      "        \"iodine\",\n",
      "        \"acetaminophen\",\n",
      "        \"pargyline HC1\",\n",
      "        \"norepinephrine\",\n",
      "        \"dopamine\",\n",
      "        \"amphetamine\",\n",
      "        \"sulfonamide\",\n",
      "        \"indoleacetic acid\",\n",
      "        \"terfenadine Antimigraine\",\n",
      "        \"gelatin\",\n",
      "        \"gelatin\",\n",
      "        \"Alternate\"\n",
      "    ],\n",
      "    \"brand\": [\n",
      "        \"MEDLINE\",\n",
      "        \"PREMEDLINE\",\n",
      "        \"Macrolides\",\n",
      "        \"ISIS-2\",\n",
      "        \"Phosphate\",\n",
      "        \"Niacin\",\n",
      "        \"Muscle\",\n",
      "        \"ANTACID\",\n",
      "        \"Phosphate\",\n",
      "        \"MAINTAIN\",\n",
      "        \"PREVENT\",\n",
      "        \"Multivitamins\",\n",
      "        \"Gleevec\"\n",
      "    ],\n",
      "    \"group\": [\n",
      "        \"polyamines\",\n",
      "        \"calcium - channel blockers\",\n",
      "        \"Slow - channel calcium blockers\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"beta-endorphin\",\n",
      "        \"opioid - antagonist\",\n",
      "        \"opioid agonist - antagonist\",\n",
      "        \"antiestrogen 4-hydroxytamoxifen\",\n",
      "        \"vitamin D receptor\",\n",
      "        \"beta-adrenergic neuroeffector\",\n",
      "        \"uptake inhibitor hydrogen-3\",\n",
      "        \"uptake blocking agents\",\n",
      "        \"Antibiotics\",\n",
      "        \"erythromycins\",\n",
      "        \"antagonist class\",\n",
      "        \"beta-blocking agents\",\n",
      "        \"Endocrine Function HMG - CoA reductase inhibitors\",\n",
      "        \"HMG - CoA reductase inhibitors\",\n",
      "        \"HMG - CoA reductase inhibitor\",\n",
      "        \"steroid hormones\",\n",
      "        \"retinogeniculate fibers\",\n",
      "        \"botulinum toxin serotypes\",\n",
      "        \"SSRIs\",\n",
      "        \"antiarrhythmics\",\n",
      "        \"systemic steroids\",\n",
      "        \"vitamin D\",\n",
      "        \"ACE ) inhibitors\",\n",
      "        \"calcium - channel blockers\",\n",
      "        \"ACTH\",\n",
      "        \"diuretic\",\n",
      "        \"loop\",\n",
      "        \"potassium - sparing\",\n",
      "        \"thiazide diuretics\",\n",
      "        \"vitamin D analogues\",\n",
      "        \"vitamin D\",\n",
      "        \"vitamin D analogues\",\n",
      "        \"vitamin D analogues\",\n",
      "        \"HMG - CoA reductase inhibitor\",\n",
      "        \"myelosuppressive agents\",\n",
      "        \"coumarin - derivative anticoagulants\",\n",
      "        \"estrogens\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"estrogens\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"Combination hormonal contraceptives\",\n",
      "        \"AED AED Felbatol\",\n",
      "        \"Contraceptives A\",\n",
      "        \"HMG - CoA reductase inhibitors\",\n",
      "        \"HMG - CoA reductase inhibitors\",\n",
      "        \"coumarin - type anticoagulants\",\n",
      "        \"Diuretics\",\n",
      "        \"Potassium - sparing diuretics\",\n",
      "        \"histamine H2 - receptor antagonists\",\n",
      "        \"anesthetic agents\",\n",
      "        \"skeletal - muscle relaxants\",\n",
      "        \"ACTH\",\n",
      "        \"diuretic\",\n",
      "        \"loop\",\n",
      "        \"triazolo-benzodiazepines\",\n",
      "        \"HMG - CoA reductase inhibitors\",\n",
      "        \"diuretic\",\n",
      "        \"loop\",\n",
      "        \"potassium - sparing\",\n",
      "        \"potassium - sparing diuretics\",\n",
      "        \"potassium - sparing diuretics\",\n",
      "        \"non-selective MAO inhibitors\",\n",
      "        \"ergot - type oxytocic drugs\",\n",
      "        \"Adrenergic blockers Adrenergic blockers\",\n",
      "        \"tricyclic Amphetamines\",\n",
      "        \"tricyclic antidepressants\",\n",
      "        \"MAO inhibitors MAOI antidepressants\",\n",
      "        \"antihypertensives\",\n",
      "        \"coumarin - type anticoagulants\",\n",
      "        \"17-ketogenic steroids\",\n",
      "        \"ergot derivatives Antimycobacterial agents\"\n",
      "    ],\n",
      "    \"drug_n\": [\n",
      "        \"phosphate\",\n",
      "        \"phosphate\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Using readlines()\n",
    "file1 = open('goldNER.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "truth = {}\n",
    "\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    value = line.split(\"|\")\n",
    "    if value[0] not in truth:\n",
    "        truth[value[0]] = []\n",
    "    truth[value[0]].append((value[-2], value[-1]))\n",
    "\n",
    "# Using readlines()\n",
    "file1 = open('../output/task9.1_out_2.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "output = {}\n",
    "wrong_entities = []\n",
    "new_sent = \"\"\n",
    "matched_entities = []\n",
    "missing = []\n",
    "\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    value = line.split(\"|\")\n",
    "    if value[0] not in output:\n",
    "        output[value[0]] = []\n",
    "    output[value[0]].append((value[-2], value[-1]))\n",
    "\n",
    "    if new_sent != value[0] and new_sent != \"\":\n",
    "        if new_sent in truth:\n",
    "            missing += [item for item in truth[new_sent] if item not in matched_entities]\n",
    "        matched_entities = []\n",
    "        new_sent = value[0]\n",
    "\n",
    "    if new_sent == \"\":\n",
    "        new_sent = value[0]\n",
    "\n",
    "    if value[0] in truth and (value[-2], value[-1]) in truth[value[0]]:\n",
    "        matched_entities.append((value[-2], value[-1]))\n",
    "    else:\n",
    "        wrong_entities.append((value[-2], value[-1]))\n",
    "\n",
    "missing_dict = {}\n",
    "for ent, type in missing:\n",
    "    if type[:-1] not in missing_dict:\n",
    "        missing_dict[type[:-1]] = []\n",
    "    missing_dict[type[:-1]].append(ent)\n",
    "\n",
    "wrong_entities_dict = {}\n",
    "for ent, type in wrong_entities:\n",
    "    if type[:-1] not in wrong_entities_dict:\n",
    "        wrong_entities_dict[type[:-1]] = []\n",
    "    wrong_entities_dict[type[:-1]].append(ent)\n",
    "\n",
    "print(\"MISSING:\\n\", json.dumps(missing_dict, indent=4))\n",
    "print(\"\\n\\n\")\n",
    "print(\"WRONG:\\n\", json.dumps(wrong_entities_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Objective completed\n",
    "- CRF comentar les seves avantatges\n",
    "- I explicar conclusions generals del Laboratori com a bloc complet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
